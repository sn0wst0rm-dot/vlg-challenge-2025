{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6777f229",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-12T14:00:37.482868Z",
     "iopub.status.busy": "2025-01-12T14:00:37.482483Z",
     "iopub.status.idle": "2025-01-12T14:00:45.586198Z",
     "shell.execute_reply": "2025-01-12T14:00:45.585038Z"
    },
    "papermill": {
     "duration": 8.108467,
     "end_time": "2025-01-12T14:00:45.587767",
     "exception": false,
     "start_time": "2025-01-12T14:00:37.479300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using OpenAI's CLIP model\n",
    "pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b92ae0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T14:00:45.593646Z",
     "iopub.status.busy": "2025-01-12T14:00:45.593272Z",
     "iopub.status.idle": "2025-01-12T14:00:45.597336Z",
     "shell.execute_reply": "2025-01-12T14:00:45.596569Z"
    },
    "papermill": {
     "duration": 0.008185,
     "end_time": "2025-01-12T14:00:45.598531",
     "exception": false,
     "start_time": "2025-01-12T14:00:45.590346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test Directory\n",
    "test_dir =\"/kaggle/input/vlg-recruitment-24-challenge/vlg-dataset/vlg-dataset/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eee1203",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T14:00:45.603475Z",
     "iopub.status.busy": "2025-01-12T14:00:45.603267Z",
     "iopub.status.idle": "2025-01-12T14:00:51.043363Z",
     "shell.execute_reply": "2025-01-12T14:00:51.042361Z"
    },
    "papermill": {
     "duration": 5.444296,
     "end_time": "2025-01-12T14:00:51.044971",
     "exception": false,
     "start_time": "2025-01-12T14:00:45.600675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing Necessary Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98db8aa3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T14:00:51.050925Z",
     "iopub.status.busy": "2025-01-12T14:00:51.050409Z",
     "iopub.status.idle": "2025-01-12T17:39:29.302568Z",
     "shell.execute_reply": "2025-01-12T17:39:29.301683Z"
    },
    "papermill": {
     "duration": 13118.256403,
     "end_time": "2025-01-12T17:39:29.303874",
     "exception": false,
     "start_time": "2025-01-12T14:00:51.047471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compiling Predicted Classes in a file\n",
    "# Initialize a list to store the prediction results\n",
    "data = []\n",
    "\n",
    "# Iterate over all files in the test directory\n",
    "for filename in os.listdir(test_dir):\n",
    "    # Check if the file is an image (based on common image extensions)\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp', '.tiff', '.webp')):\n",
    "        # Get the full path of the image\n",
    "        image_path = os.path.join(test_dir, filename)\n",
    "        # Check if a GPU is available, otherwise use CPU for computations\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        # Load the CLIP model (ViT-B/32 architecture) and its preprocessing pipeline\n",
    "        model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "         # Preprocess the image (resize, normalize, etc.) and prepare it as a batch\n",
    "        image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "        # Define the set of class labels to classify the images into\n",
    "        labels =[\"antelope\", \"grizzly bear\", \"killer whale\", \"beaver\", \"dalmatian\", \"persian cat\", \"horse\", \"german shepherd\", \"blue whale\", \"siamese cat\", \"skunk\", \"mole\", \"tiger\", \"hippopotamus\", \"leopard\", \"moose\", \"spider monkey\", \"humpback whale\", \"elephant\", \"gorilla\", \"ox\", \"fox\", \"sheep\", \"seal\", \"chimpanzee\", \"hamster\", \"squirrel\", \"rhinoceros\", \"rabbit\", \"bat\", \"giraffe\", \"wolf\", \"chihuahua\", \"rat\", \"weasel\", \"otter\", \"buffalo\", \"zebra\", \"giant panda\", \"deer\", \"bobcat\", \"pig\", \"lion\", \"mouse\", \"polar bear\", \"collie\", \"walrus\", \"raccoon\", \"cow\", \"dolphin\"]\n",
    "        # Tokenize the text labels (convert them into embeddings the model can understand)\n",
    "        text = clip.tokenize([\"antelope\", \"grizzly bear\", \"killer whale\", \"beaver\", \"dalmatian\", \"persian cat\", \"horse\", \"german shepherd\", \"blue whale\", \"siamese cat\", \"skunk\", \"mole\", \"tiger\", \"hippopotamus\", \"leopard\", \"moose\", \"spider monkey\", \"humpback whale\", \"elephant\", \"gorilla\", \"ox\", \"fox\", \"sheep\", \"seal\", \"chimpanzee\", \"hamster\", \"squirrel\", \"rhinoceros\", \"rabbit\", \"bat\", \"giraffe\", \"wolf\", \"chihuahua\", \"rat\", \"weasel\", \"otter\", \"buffalo\", \"zebra\", \"giant panda\", \"deer\", \"bobcat\", \"pig\", \"lion\", \"mouse\", \"polar bear\", \"collie\", \"walrus\", \"raccoon\", \"cow\", \"dolphin\"]).to(device)\n",
    "        # Perform inference using the CLIP model\n",
    "        with torch.no_grad():\n",
    "            # Compute image features\n",
    "            image_features = model.encode_image(image)\n",
    "            # Compute text features\n",
    "            text_features = model.encode_text(text)\n",
    "            # Calculate similarity scores between the image and text labels\n",
    "            logits_per_image, logits_per_text = model(image, text)\n",
    "        # Compute probabilities from the similarity scores\n",
    "        preds = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "        # Get the class label with the highest probability\n",
    "        predicted_class = labels[preds.argmax()]\n",
    "        print({'image_id': filename, 'class': predicted_class})\n",
    "        data.append({'image_id': filename, 'class': predicted_class})\n",
    "# Create a Pandas DataFrame from the prediction results\n",
    "submission = pd.DataFrame(data)\n",
    "# Save the predictions to a CSV file for submission\n",
    "submission.to_csv('/kaggle/working/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 10740331,
     "sourceId": 90860,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13135.784788,
   "end_time": "2025-01-12T17:39:31.072018",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-12T14:00:35.287230",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
